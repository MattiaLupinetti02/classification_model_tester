{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062fb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_model_tester import ModelTester\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76fed9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_1 = pd.read_csv(\"../dati_dott_fedele/dati_m2_incolonnati.csv\")\n",
    "dt_2 = pd.read_csv(\"../dati_dott_fedele/dati_m2_incolonnati_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e4d08",
   "metadata": {},
   "source": [
    "#### Test n.1: dataset without the feature 'previous_alert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3463c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score,f1_score\n",
    "\n",
    "models = {\n",
    "    # LogisticRegression\n",
    "    LogisticRegression(max_iter=10000000) : { \n",
    "        'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "        'C': np.logspace(-3, 9, 7),\n",
    "        'solver': ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga'],\n",
    "        'tol': np.logspace(-5, -1, 5)\n",
    "    },\n",
    "\n",
    "    # SVC\n",
    "    SVC(probability=True) : { \n",
    "        'kernel': ['rbf', 'poly', 'linear'],\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'degree': [2, 3, 4, 5]  \n",
    "    }\n",
    "   \n",
    "}\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "resampling_methods = [\n",
    "    SMOTE(random_state=42, sampling_strategy='auto'),\n",
    "    ADASYN(random_state=42, sampling_strategy='minority')\n",
    "]\n",
    "metrics = {\"accuracy\":accuracy_score,\"precision\":precision_score,\"recall\":recall_score,\"f1-score\":f1_score}\n",
    "\n",
    "tester_exp1 = ModelTester(models,metrics,dt_1.copy(),\"alert\",resamplingMethods=resampling_methods,ensambleModelList=[StackingClassifier(estimators=None)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00685c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d1db8026a84a20b47e7d73bba08b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run monitor_resource.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff56e1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: LogisticRegression(max_iter=10000000) with GridSearchCV\n",
      "Fitting 2 folds for each of 700 candidates, totalling 1400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "910 fits failed out of a total of 1400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "350 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.60166667 0.60166667 0.60166667 0.60166667 0.60166667        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.60166667 0.60166667 0.60166667\n",
      " 0.60166667 0.60166667 0.60166667 0.60166667 0.60166667 0.60166667\n",
      " 0.60166667 0.76833333 0.76833333 0.76833333 0.76833333 0.77166667\n",
      " 0.60166667 0.60166667 0.60166667 0.60166667 0.60166667 0.60166667\n",
      " 0.60166667 0.60166667 0.60166667 0.60166667 0.60166667 0.60166667\n",
      " 0.60166667 0.60166667 0.60166667        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.76666667 0.76666667\n",
      " 0.76666667 0.76666667 0.76666667        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.78166667 0.78166667 0.78166667 0.77666667 0.77333333\n",
      " 0.785      0.785      0.78333333 0.78       0.76833333 0.765\n",
      " 0.765      0.765      0.765      0.76666667 0.785      0.785\n",
      " 0.785      0.77666667 0.77166667 0.785      0.785      0.785\n",
      " 0.78333333 0.78166667 0.785      0.785      0.78333333 0.775\n",
      " 0.76166667        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.77666667 0.77666667 0.77666667 0.77666667\n",
      " 0.77666667        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.81833333\n",
      " 0.81833333 0.81666667 0.81333333 0.785      0.81833333 0.81833333\n",
      " 0.815      0.81166667 0.77       0.77666667 0.77666667 0.77666667\n",
      " 0.77666667 0.77333333 0.81833333 0.81833333 0.815      0.81\n",
      " 0.77166667 0.81833333 0.81833333 0.81666667 0.815      0.78833333\n",
      " 0.81833333 0.81833333 0.815      0.81333333 0.78333333        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.77666667 0.77666667 0.77666667 0.775      0.77833333        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.81666667 0.81666667 0.81666667\n",
      " 0.81       0.775      0.81666667 0.81833333 0.815      0.81166667\n",
      " 0.77       0.77666667 0.77666667 0.77666667 0.77833333 0.77333333\n",
      " 0.81666667 0.81833333 0.81666667 0.81       0.77166667 0.81666667\n",
      " 0.81666667 0.81833333 0.815      0.80666667 0.81666667 0.81666667\n",
      " 0.81833333 0.81       0.78333333        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.77666667 0.77666667\n",
      " 0.77666667 0.775      0.78              nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.81666667 0.815      0.81833333 0.81       0.78166667\n",
      " 0.81666667 0.81833333 0.815      0.81166667 0.77       0.77666667\n",
      " 0.77666667 0.77666667 0.77833333 0.77333333 0.81666667 0.81833333\n",
      " 0.81666667 0.81       0.77166667 0.81666667 0.81666667 0.81666667\n",
      " 0.81166667 0.80833333 0.815      0.815      0.81833333 0.81166667\n",
      " 0.78166667        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.77666667 0.77666667 0.77666667 0.775\n",
      " 0.77666667        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.81666667\n",
      " 0.81666667 0.815      0.81166667 0.78       0.81666667 0.81833333\n",
      " 0.815      0.81166667 0.77       0.77666667 0.77666667 0.77666667\n",
      " 0.77833333 0.77333333 0.81666667 0.81833333 0.81666667 0.81\n",
      " 0.77166667 0.81666667 0.81666667 0.81833333 0.815      0.80166667\n",
      " 0.81666667 0.81666667 0.81666667 0.81166667 0.78333333        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.77666667 0.77666667 0.77666667 0.77666667 0.77333333        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.81666667 0.81666667 0.81666667\n",
      " 0.81166667 0.795      0.81666667 0.81833333 0.815      0.81166667\n",
      " 0.77       0.77666667 0.77666667 0.77666667 0.77833333 0.77333333\n",
      " 0.81666667 0.81833333 0.81666667 0.81       0.77166667 0.81666667\n",
      " 0.81666667 0.81833333 0.815      0.78833333 0.81666667 0.81666667\n",
      " 0.81833333 0.81166667 0.78833333        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.20055556 0.20055556 0.20055556 0.20055556 0.20055556        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.20055556 0.20055556 0.20055556\n",
      " 0.20055556 0.20055556 0.20055556 0.20055556 0.20055556 0.20055556\n",
      " 0.20055556 0.67248365 0.67248365 0.67248365 0.67248365 0.68245316\n",
      " 0.20055556 0.20055556 0.20055556 0.20055556 0.20055556 0.20055556\n",
      " 0.20055556 0.20055556 0.20055556 0.20055556 0.20055556 0.20055556\n",
      " 0.20055556 0.20055556 0.20055556        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.67220251 0.67220251\n",
      " 0.67220251 0.67220251 0.67220251        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.69720289 0.69720289 0.69720289 0.68586957 0.67209112\n",
      " 0.71770685 0.71770685 0.71504335 0.70479327 0.68598687 0.66518121\n",
      " 0.66518121 0.66518121 0.66518121 0.67584955 0.71770685 0.71770685\n",
      " 0.71770685 0.6956103  0.68245316 0.71770685 0.71770685 0.71770685\n",
      " 0.713933   0.70112991 0.71770685 0.71770685 0.71546202 0.69304359\n",
      " 0.6631887         nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.69348458 0.69348458 0.69348458 0.69348458\n",
      " 0.69155612        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.75889153\n",
      " 0.75889153 0.75587062 0.74907388 0.70065144 0.75889153 0.75889153\n",
      " 0.75439646 0.74620453 0.66136647 0.69348458 0.69348458 0.69348458\n",
      " 0.69348458 0.68520404 0.75889153 0.75889153 0.75439646 0.74410482\n",
      " 0.68245316 0.75889153 0.75889153 0.75678942 0.75436767 0.7129257\n",
      " 0.75889153 0.75889153 0.75369467 0.74874055 0.70447944        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69597453 0.69597453 0.69597453 0.69006437 0.69793281        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.75798078 0.75670262 0.75709341\n",
      " 0.74378095 0.67547353 0.75798078 0.7608524  0.75638188 0.74620453\n",
      " 0.66136647 0.69597453 0.69597453 0.69597453 0.69939474 0.68358573\n",
      " 0.75798078 0.7608524  0.75845944 0.74410482 0.68245316 0.75798078\n",
      " 0.75798078 0.75948637 0.75254835 0.74153566 0.75798078 0.75798078\n",
      " 0.75948637 0.74378095 0.69301677        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.69597453 0.69597453\n",
      " 0.69597453 0.69006437 0.70524799        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.75798078 0.75430966 0.75948637 0.74336017 0.69902097\n",
      " 0.75798078 0.7608524  0.75638188 0.74620453 0.66136647 0.69597453\n",
      " 0.69597453 0.69597453 0.69939474 0.68358573 0.75798078 0.7608524\n",
      " 0.75845944 0.74410482 0.68245316 0.75798078 0.75798078 0.75670262\n",
      " 0.75022882 0.74174936 0.75558782 0.75558782 0.75925877 0.74590007\n",
      " 0.6933836         nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.69597453 0.69597453 0.69597453 0.69006437\n",
      " 0.69190138        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.75798078\n",
      " 0.75798078 0.75430966 0.74663843 0.68263967 0.75798078 0.7608524\n",
      " 0.75638188 0.74620453 0.66136647 0.69597453 0.69597453 0.69597453\n",
      " 0.69939474 0.68358573 0.75798078 0.7608524  0.75845944 0.74410482\n",
      " 0.68245316 0.75798078 0.75798078 0.7608524  0.75430966 0.73555302\n",
      " 0.75798078 0.75798078 0.75709341 0.74590007 0.69388579        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69597453 0.69597453 0.69597453 0.69348458 0.68597625        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.75798078 0.75670262 0.75709341\n",
      " 0.74590007 0.71796462 0.75798078 0.7608524  0.75638188 0.74620453\n",
      " 0.66136647 0.69597453 0.69597453 0.69597453 0.69939474 0.68358573\n",
      " 0.75798078 0.7608524  0.75845944 0.74410482 0.68245316 0.75798078\n",
      " 0.75798078 0.75948637 0.75396851 0.71170863 0.75798078 0.75798078\n",
      " 0.7606248  0.74620065 0.69924056        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.63564694 0.63564694 0.63564694 0.63564694 0.64111142\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.63424834 0.63424834\n",
      " 0.63424834 0.63424834 0.63424834        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.65240224 0.65240224 0.65240224 0.6458266  0.64263664\n",
      " 0.66848751 0.66848751 0.66566265 0.66196089 0.63787243 0.63393673\n",
      " 0.63393673 0.63393673 0.63393673 0.63485754 0.66848751 0.66848751\n",
      " 0.66848751 0.65631117 0.64111142 0.66848751 0.66848751 0.66848751\n",
      " 0.66566265 0.66149424 0.66848751 0.66848751 0.6675667  0.65334499\n",
      " 0.63102909        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.65593591 0.65593591 0.65593591 0.65593591\n",
      " 0.65593591        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.73499643\n",
      " 0.73499643 0.73226419 0.72651707 0.67782219 0.73499643 0.73499643\n",
      " 0.73125076 0.72567864 0.64446147 0.65593591 0.65593591 0.65593591\n",
      " 0.65593591 0.65014487 0.73499643 0.73499643 0.73125076 0.72286402\n",
      " 0.64111142 0.73499643 0.73499643 0.73217157 0.72748658 0.69015317\n",
      " 0.73499643 0.73499643 0.73134338 0.72651707 0.67119435        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.65607723 0.65607723 0.65607723 0.65320367 0.65843421        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.73407051 0.73407051 0.73588194\n",
      " 0.72095997 0.66147266 0.73407051 0.73680275 0.73305708 0.72567864\n",
      " 0.64446147 0.65607723 0.65607723 0.65607723 0.65880947 0.65023749\n",
      " 0.73407051 0.73680275 0.73588194 0.72286402 0.64111142 0.73407051\n",
      " 0.73407051 0.73680275 0.73120206 0.71878123 0.73407051 0.73407051\n",
      " 0.73680275 0.72095997 0.6731859         nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.65607723 0.65607723\n",
      " 0.65607723 0.65320367 0.66326053        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73407051 0.7331497  0.73680275 0.72286402 0.67221606\n",
      " 0.73407051 0.73680275 0.73305708 0.72567864 0.64446147 0.65607723\n",
      " 0.65607723 0.65607723 0.65880947 0.65023749 0.73407051 0.73680275\n",
      " 0.73588194 0.72286402 0.64111142 0.73407051 0.73407051 0.73407051\n",
      " 0.72578117 0.71646979 0.7331497  0.7331497  0.7387555  0.72378483\n",
      " 0.67245032        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.65607723 0.65607723 0.65607723 0.65320367\n",
      " 0.65760602        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.73407051\n",
      " 0.73407051 0.7331497  0.72369221 0.66615895 0.73407051 0.73680275\n",
      " 0.73305708 0.72567864 0.64446147 0.65607723 0.65607723 0.65607723\n",
      " 0.65880947 0.65023749 0.73407051 0.73680275 0.73588194 0.72286402\n",
      " 0.64111142 0.73407051 0.73407051 0.73680275 0.7331497  0.71638416\n",
      " 0.73407051 0.73407051 0.73588194 0.72378483 0.66957327        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.65607723 0.65607723 0.65607723 0.65593591 0.65209762        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.73407051 0.73407051 0.73588194\n",
      " 0.72378483 0.69486875 0.73407051 0.73680275 0.73305708 0.72567864\n",
      " 0.64446147 0.65607723 0.65607723 0.65607723 0.65880947 0.65023749\n",
      " 0.73407051 0.73680275 0.73588194 0.72286402 0.64111142 0.73407051\n",
      " 0.73407051 0.73680275 0.72943933 0.69795842 0.73407051 0.73407051\n",
      " 0.7387555  0.72559626 0.68138773        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\mlupi\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.25043313 0.25043313 0.25043313 0.25043313 0.25043313        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.25043313 0.25043313 0.25043313\n",
      " 0.25043313 0.25043313 0.25043313 0.25043313 0.25043313 0.25043313\n",
      " 0.25043313 0.60730222 0.60730222 0.60730222 0.60730222 0.61135326\n",
      " 0.25043313 0.25043313 0.25043313 0.25043313 0.25043313 0.25043313\n",
      " 0.25043313 0.25043313 0.25043313 0.25043313 0.25043313 0.25043313\n",
      " 0.25043313 0.25043313 0.25043313        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.59487477 0.59487477\n",
      " 0.59487477 0.59487477 0.59487477        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.63437687 0.63437687 0.63437687 0.62492269 0.62982483\n",
      " 0.67357408 0.67357408 0.67055066 0.66511589 0.62289819 0.61310179\n",
      " 0.61310179 0.61310179 0.61310179 0.61511087 0.67357408 0.67357408\n",
      " 0.67357408 0.65758134 0.61135326 0.67357408 0.67357408 0.67357408\n",
      " 0.66997428 0.66847466 0.67357408 0.67357408 0.67206754 0.65181505\n",
      " 0.62421712        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.65231872 0.65231872 0.65231872 0.65231872\n",
      " 0.65199141        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.7432786\n",
      " 0.7432786  0.74090824 0.73344781 0.67423962 0.7432786  0.7432786\n",
      " 0.73929862 0.73254309 0.62714703 0.65231872 0.65231872 0.65231872\n",
      " 0.65231872 0.64290916 0.7432786  0.7432786  0.73929862 0.73019847\n",
      " 0.61135326 0.7432786  0.7432786  0.74063623 0.73644382 0.6962097\n",
      " 0.7432786  0.7432786  0.73944821 0.73337448 0.66919078        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.6546935  0.6546935  0.6546935  0.64996966 0.65071574        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.74236032 0.74190362 0.74303394\n",
      " 0.72823159 0.66119141 0.74236032 0.74484466 0.74085469 0.73254309\n",
      " 0.62714703 0.6546935  0.6546935  0.6546935  0.65704256 0.64455264\n",
      " 0.74236032 0.74484466 0.74350705 0.73019847 0.61135326 0.74236032\n",
      " 0.74236032 0.74437155 0.73842445 0.72454339 0.74236032 0.74236032\n",
      " 0.74437155 0.72823159 0.66813048        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.6546935  0.6546935\n",
      " 0.6546935  0.64996966 0.65768333        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.74236032 0.74056601 0.74437155 0.72953881 0.66796773\n",
      " 0.74236032 0.74484466 0.74085469 0.73254309 0.62714703 0.6546935\n",
      " 0.6546935  0.6546935  0.65704256 0.64455264 0.74236032 0.74484466\n",
      " 0.74350705 0.73019847 0.61135326 0.74236032 0.74236032 0.74190362\n",
      " 0.73460938 0.72541989 0.74102271 0.74102271 0.74579259 0.73086152\n",
      " 0.67208558        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.6546935  0.6546935  0.6546935  0.64996966\n",
      " 0.64836001        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.74236032\n",
      " 0.74236032 0.74056601 0.73073212 0.66499114 0.74236032 0.74484466\n",
      " 0.74085469 0.73254309 0.62714703 0.6546935  0.6546935  0.6546935\n",
      " 0.65704256 0.64455264 0.74236032 0.74484466 0.74350705 0.73019847\n",
      " 0.61135326 0.74236032 0.74236032 0.74484466 0.74056601 0.72336964\n",
      " 0.74236032 0.74236032 0.74303394 0.73086152 0.66671048        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.6546935  0.6546935  0.6546935  0.65231872 0.6456996         nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.74236032 0.74190362 0.74303394\n",
      " 0.73086152 0.69501426 0.74236032 0.74484466 0.74085469 0.73254309\n",
      " 0.62714703 0.6546935  0.6546935  0.6546935  0.65704256 0.64455264\n",
      " 0.74236032 0.74484466 0.74350705 0.73019847 0.61135326 0.74236032\n",
      " 0.74236032 0.74437155 0.73815589 0.70270839 0.74236032 0.74236032\n",
      " 0.7462657  0.73205177 0.67555342        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest parameters for accuracy: {'C': 10.0, 'penalty': 'l1', 'solver': 'saga', 'tol': 1e-05}\n",
      "\tBest score for accuracy: 0.8183333333333334\n",
      "\tBest parameters for precision: {'C': 10.0, 'penalty': 'l1', 'solver': 'saga', 'tol': 1e-05}\n",
      "\tBest score for precision: 0.7588915295637573\n",
      "\tBest parameters for recall: {'C': 10.0, 'penalty': 'l1', 'solver': 'saga', 'tol': 1e-05}\n",
      "\tBest score for recall: 0.7349964329369911\n",
      "\tBest parameters for f1-score: {'C': 10.0, 'penalty': 'l1', 'solver': 'saga', 'tol': 1e-05}\n",
      "\tBest score for f1-score: 0.7432785972121638\n",
      "Testing model: SVC(probability=True) with GridSearchCV\n",
      "Fitting 2 folds for each of 168 candidates, totalling 336 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest parameters for accuracy: {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for accuracy: 0.815\n",
      "\tBest parameters for precision: {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for precision: 0.7564208607370386\n",
      "\tBest parameters for recall: {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for recall: 0.7394117863163276\n",
      "\tBest parameters for f1-score: {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for f1-score: 0.7464212888064341\n"
     ]
    }
   ],
   "source": [
    "tester_exp1.best_param_calculator(cv=2,avg=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e541598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': {'LogisticRegression': {'accuracy': {'{\"C\": 10.0, \"penalty\": \"l1\", \"solver\": \"saga\", \"tol\": 1e-05}': 0.8183333333333334},\n",
       "   'precision': {'{\"C\": 10.0, \"penalty\": \"l1\", \"solver\": \"saga\", \"tol\": 1e-05}': 0.7588915295637573},\n",
       "   'recall': {'{\"C\": 10.0, \"penalty\": \"l1\", \"solver\": \"saga\", \"tol\": 1e-05}': 0.7349964329369911},\n",
       "   'f1-score': {'{\"C\": 10.0, \"penalty\": \"l1\", \"solver\": \"saga\", \"tol\": 1e-05}': 0.7432785972121638}},\n",
       "  'SVC': {'accuracy': {'{\"C\": 1, \"degree\": 2, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.815},\n",
       "   'precision': {'{\"C\": 1, \"degree\": 2, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.7564208607370386},\n",
       "   'recall': {'{\"C\": 1, \"degree\": 2, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.7394117863163276},\n",
       "   'f1-score': {'{\"C\": 1, \"degree\": 2, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.7464212888064341}}},\n",
       " 'specific': {'LogisticRegression': {'accuracy_GREEN': None,\n",
       "   'accuracy_RED': None,\n",
       "   'accuracy_YELLOW': None,\n",
       "   'precision_GREEN': None,\n",
       "   'precision_RED': None,\n",
       "   'precision_YELLOW': None,\n",
       "   'recall_GREEN': None,\n",
       "   'recall_RED': None,\n",
       "   'recall_YELLOW': None,\n",
       "   'f1-score_GREEN': None,\n",
       "   'f1-score_RED': None,\n",
       "   'f1-score_YELLOW': None},\n",
       "  'SVC': {'accuracy_GREEN': None,\n",
       "   'accuracy_RED': None,\n",
       "   'accuracy_YELLOW': None,\n",
       "   'precision_GREEN': None,\n",
       "   'precision_RED': None,\n",
       "   'precision_YELLOW': None,\n",
       "   'recall_GREEN': None,\n",
       "   'recall_RED': None,\n",
       "   'recall_YELLOW': None,\n",
       "   'f1-score_GREEN': None,\n",
       "   'f1-score_RED': None,\n",
       "   'f1-score_YELLOW': None}},\n",
       " 'overall_SMOTE': {'LogisticRegression': {'accuracy': None,\n",
       "   'precision': None,\n",
       "   'recall': None,\n",
       "   'f1-score': None},\n",
       "  'SVC': {'accuracy': None,\n",
       "   'precision': None,\n",
       "   'recall': None,\n",
       "   'f1-score': None}},\n",
       " 'specific_SMOTE': {'LogisticRegression': {'accuracy_GREEN': None,\n",
       "   'accuracy_RED': None,\n",
       "   'accuracy_YELLOW': None,\n",
       "   'precision_GREEN': None,\n",
       "   'precision_RED': None,\n",
       "   'precision_YELLOW': None,\n",
       "   'recall_GREEN': None,\n",
       "   'recall_RED': None,\n",
       "   'recall_YELLOW': None,\n",
       "   'f1-score_GREEN': None,\n",
       "   'f1-score_RED': None,\n",
       "   'f1-score_YELLOW': None},\n",
       "  'SVC': {'accuracy_GREEN': None,\n",
       "   'accuracy_RED': None,\n",
       "   'accuracy_YELLOW': None,\n",
       "   'precision_GREEN': None,\n",
       "   'precision_RED': None,\n",
       "   'precision_YELLOW': None,\n",
       "   'recall_GREEN': None,\n",
       "   'recall_RED': None,\n",
       "   'recall_YELLOW': None,\n",
       "   'f1-score_GREEN': None,\n",
       "   'f1-score_RED': None,\n",
       "   'f1-score_YELLOW': None}},\n",
       " 'overall_ADASYN': {'LogisticRegression': {'accuracy': None,\n",
       "   'precision': None,\n",
       "   'recall': None,\n",
       "   'f1-score': None},\n",
       "  'SVC': {'accuracy': None,\n",
       "   'precision': None,\n",
       "   'recall': None,\n",
       "   'f1-score': None}},\n",
       " 'specific_ADASYN': {'LogisticRegression': {'accuracy_GREEN': None,\n",
       "   'accuracy_RED': None,\n",
       "   'accuracy_YELLOW': None,\n",
       "   'precision_GREEN': None,\n",
       "   'precision_RED': None,\n",
       "   'precision_YELLOW': None,\n",
       "   'recall_GREEN': None,\n",
       "   'recall_RED': None,\n",
       "   'recall_YELLOW': None,\n",
       "   'f1-score_GREEN': None,\n",
       "   'f1-score_RED': None,\n",
       "   'f1-score_YELLOW': None},\n",
       "  'SVC': {'accuracy_GREEN': None,\n",
       "   'accuracy_RED': None,\n",
       "   'accuracy_YELLOW': None,\n",
       "   'precision_GREEN': None,\n",
       "   'precision_RED': None,\n",
       "   'precision_YELLOW': None,\n",
       "   'recall_GREEN': None,\n",
       "   'recall_RED': None,\n",
       "   'recall_YELLOW': None,\n",
       "   'f1-score_GREEN': None,\n",
       "   'f1-score_RED': None,\n",
       "   'f1-score_YELLOW': None}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63632982",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'remove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tester_exp1\u001b[38;5;241m.\u001b[39mplot_model_performance_histograms(performances)\n",
      "File \u001b[1;32mc:\\Users\\mlupi\\OneDrive - Universit degli Studi dell'Aquila\\Documenti\\tesi\\notebooks\\classification_model_tester.py:572\u001b[0m, in \u001b[0;36mModelTester.plot_model_performance_histograms\u001b[1;34m(self, data, save, name)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# Creiamo il barplot per il modello corrente, separando le metriche\u001b[39;00m\n\u001b[0;32m    570\u001b[0m sns\u001b[38;5;241m.\u001b[39mbarplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric/Class\u001b[39m\u001b[38;5;124m\"\u001b[39m,y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerformance\u001b[39m\u001b[38;5;124m\"\u001b[39m,hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric/Class\u001b[39m\u001b[38;5;124m\"\u001b[39m,data\u001b[38;5;241m=\u001b[39mmodel_data,ax\u001b[38;5;241m=\u001b[39max,dodge\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m ) \u001b[38;5;66;03m# Dividiamo le barre per iperparametro\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m ax\u001b[38;5;241m.\u001b[39mget_legend()\u001b[38;5;241m.\u001b[39mremove()\n\u001b[0;32m    573\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerformance for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    574\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerformance\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'remove'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tester_exp1.plot_model_performance_histograms(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db55ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: StackingClassifier(estimators=[('LogisticRegression', LogisticRegression()),\n",
      "                               ('SVC', SVC(probability=True))]) with GridSearchCV\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest parameters for f1_GREEN: {'LogisticRegression__C': 1, 'LogisticRegression__max_iter': 100, 'SVC__C': 1, 'SVC__gamma': 'scale', 'SVC__kernel': 'linear'}\n",
      "\tBest score for f1_GREEN: 0.9101078741337009\n",
      "\tBest parameters for f1_RED: {'LogisticRegression__C': 1, 'LogisticRegression__max_iter': 100, 'SVC__C': 1, 'SVC__gamma': 'scale', 'SVC__kernel': 'linear'}\n",
      "\tBest score for f1_RED: 0.8389999999999999\n",
      "\tBest parameters for f1_YELLOW: {'LogisticRegression__C': 1, 'LogisticRegression__max_iter': 100, 'SVC__C': 1, 'SVC__gamma': 'scale', 'SVC__kernel': 'linear'}\n",
      "\tBest score for f1_YELLOW: 0.5712039072039071\n",
      "\tBest parameters for precision_GREEN: {'LogisticRegression__C': 1, 'LogisticRegression__max_iter': 100, 'SVC__C': 1, 'SVC__gamma': 'scale', 'SVC__kernel': 'linear'}\n",
      "\tBest score for precision_GREEN: 0.8847615018798203\n",
      "\tBest parameters for precision_RED: {'LogisticRegression__C': 1, 'LogisticRegression__max_iter': 100, 'SVC__C': 1, 'SVC__gamma': 'scale', 'SVC__kernel': 'linear'}\n",
      "\tBest score for precision_RED: 0.8127435897435898\n",
      "\tBest parameters for precision_YELLOW: {'LogisticRegression__C': 1, 'LogisticRegression__max_iter': 100, 'SVC__C': 1, 'SVC__gamma': 'scale', 'SVC__kernel': 'linear'}\n",
      "\tBest score for precision_YELLOW: 0.6893162393162393\n",
      "\tBest parameters for recall_GREEN: {'LogisticRegression__C': 1, 'LogisticRegression__max_iter': 100, 'SVC__C': 1, 'SVC__gamma': 'scale', 'SVC__kernel': 'linear'}\n",
      "\tBest score for recall_GREEN: 0.9390030441400304\n",
      "\tBest parameters for recall_RED: {'LogisticRegression__C': 1, 'LogisticRegression__max_iter': 100, 'SVC__C': 1, 'SVC__gamma': 'scale', 'SVC__kernel': 'linear'}\n",
      "\tBest score for recall_RED: 0.869\n",
      "\tBest parameters for recall_YELLOW: {'LogisticRegression__C': 1, 'LogisticRegression__max_iter': 100, 'SVC__C': 1, 'SVC__gamma': 'scale', 'SVC__kernel': 'linear'}\n",
      "\tBest score for recall_YELLOW: 0.5018115942028986\n"
     ]
    }
   ],
   "source": [
    "tester_exp1.best_param_calculator_ensamble_by_label(cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0548ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for augmented data with SMOTE\n",
      "            Age  Gender_M  fumatore?  ex-fumatore?  non-fumatore?  \\\n",
      "0     43.000000         0          0             0              1   \n",
      "1     79.000000         1          0             1              0   \n",
      "2     81.000000         1          1             0              0   \n",
      "3     81.000000         1          0             0              1   \n",
      "4     48.000000         0          0             0              1   \n",
      "...         ...       ...        ...           ...            ...   \n",
      "1078  33.126066         0          0             0              1   \n",
      "1079  86.287093         1          0             0              1   \n",
      "1080  81.764948         1          0             0              1   \n",
      "1081  34.753948         1          1             0              0   \n",
      "1082  58.714404         0          0             0              1   \n",
      "\n",
      "      previous_alert_GREEN  previous_alert_RED  previous_alert_YELLOW  \\\n",
      "0                        0                   0                      0   \n",
      "1                        0                   0                      0   \n",
      "2                        0                   0                      0   \n",
      "3                        0                   0                      0   \n",
      "4                        0                   0                      0   \n",
      "...                    ...                 ...                    ...   \n",
      "1078                     0                   0                      1   \n",
      "1079                     0                   0                      0   \n",
      "1080                     0                   0                      0   \n",
      "1081                     0                   0                      0   \n",
      "1082                     0                   0                      0   \n",
      "\n",
      "              FC       SpO2  oxigen_therapy_ox  oxigen_therapy_venturi_mask  \\\n",
      "0     104.000000  94.000000                  0                            0   \n",
      "1      85.000000  89.000000                  0                            0   \n",
      "2     114.000000  86.000000                  0                            0   \n",
      "3      92.000000  86.000000                  0                            0   \n",
      "4      87.000000  93.000000                  0                            0   \n",
      "...          ...        ...                ...                          ...   \n",
      "1078  154.085689  94.943697                  0                            0   \n",
      "1079  125.762364  98.168275                  0                            0   \n",
      "1080   88.470105  93.764948                  1                            0   \n",
      "1081  101.544472  96.544472                  1                            0   \n",
      "1082  118.269776  91.015820                  0                            0   \n",
      "\n",
      "      6MWT         TC  \n",
      "0        0  36.000000  \n",
      "1        0  36.300000  \n",
      "2        0  36.200000  \n",
      "3        0  36.500000  \n",
      "4        0  36.000000  \n",
      "...    ...        ...  \n",
      "1078     0  36.005630  \n",
      "1079     0  36.335645  \n",
      "1080     0  38.564948  \n",
      "1081     0  37.298421  \n",
      "1082     1  37.293672  \n",
      "\n",
      "[1083 rows x 14 columns]\n",
      "Testing model: StackingClassifier(estimators=[('LogisticRegression', LogisticRegression()),\n",
      "                               ('SVC', SVC(probability=True))]) with GridSearchCV\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest parameters for f1_GREEN: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for f1_GREEN: 0.916383317106266\n",
      "\tBest parameters for f1_RED: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for f1_RED: 0.9169952555306375\n",
      "\tBest parameters for f1_YELLOW: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for f1_YELLOW: 0.8516679786160747\n",
      "\tBest parameters for precision_GREEN: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for precision_GREEN: 0.9365315350568661\n",
      "\tBest parameters for precision_RED: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for precision_RED: 0.9214291857770119\n",
      "\tBest parameters for precision_YELLOW: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for precision_YELLOW: 0.8344365819177003\n",
      "\tBest parameters for recall_GREEN: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for recall_GREEN: 0.8975266362252663\n",
      "\tBest parameters for recall_RED: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for recall_RED: 0.9141933028919331\n",
      "\tBest parameters for recall_YELLOW: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for recall_YELLOW: 0.8724124809741248\n",
      "for augmented data with ADASYN\n",
      "            Age  Gender_M  fumatore?  ex-fumatore?  non-fumatore?  \\\n",
      "0     43.000000         0          0             0              1   \n",
      "1     79.000000         1          0             1              0   \n",
      "2     81.000000         1          1             0              0   \n",
      "3     81.000000         1          0             0              1   \n",
      "4     48.000000         0          0             0              1   \n",
      "...         ...       ...        ...           ...            ...   \n",
      "1078  33.126066         0          0             0              1   \n",
      "1079  86.287093         1          0             0              1   \n",
      "1080  81.764948         1          0             0              1   \n",
      "1081  34.753948         1          1             0              0   \n",
      "1082  58.714404         0          0             0              1   \n",
      "\n",
      "      previous_alert_GREEN  previous_alert_RED  previous_alert_YELLOW  \\\n",
      "0                        0                   0                      0   \n",
      "1                        0                   0                      0   \n",
      "2                        0                   0                      0   \n",
      "3                        0                   0                      0   \n",
      "4                        0                   0                      0   \n",
      "...                    ...                 ...                    ...   \n",
      "1078                     0                   0                      1   \n",
      "1079                     0                   0                      0   \n",
      "1080                     0                   0                      0   \n",
      "1081                     0                   0                      0   \n",
      "1082                     0                   0                      0   \n",
      "\n",
      "              FC       SpO2  oxigen_therapy_ox  oxigen_therapy_venturi_mask  \\\n",
      "0     104.000000  94.000000                  0                            0   \n",
      "1      85.000000  89.000000                  0                            0   \n",
      "2     114.000000  86.000000                  0                            0   \n",
      "3      92.000000  86.000000                  0                            0   \n",
      "4      87.000000  93.000000                  0                            0   \n",
      "...          ...        ...                ...                          ...   \n",
      "1078  154.085689  94.943697                  0                            0   \n",
      "1079  125.762364  98.168275                  0                            0   \n",
      "1080   88.470105  93.764948                  1                            0   \n",
      "1081  101.544472  96.544472                  1                            0   \n",
      "1082  118.269776  91.015820                  0                            0   \n",
      "\n",
      "      6MWT         TC  \n",
      "0        0  36.000000  \n",
      "1        0  36.300000  \n",
      "2        0  36.200000  \n",
      "3        0  36.500000  \n",
      "4        0  36.000000  \n",
      "...    ...        ...  \n",
      "1078     0  36.005630  \n",
      "1079     0  36.335645  \n",
      "1080     0  38.564948  \n",
      "1081     0  37.298421  \n",
      "1082     1  37.293672  \n",
      "\n",
      "[1083 rows x 14 columns]\n",
      "Testing model: StackingClassifier(estimators=[('LogisticRegression', LogisticRegression()),\n",
      "                               ('SVC', SVC(probability=True))]) with GridSearchCV\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest parameters for f1_GREEN: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for f1_GREEN: 0.9166155022218897\n",
      "\tBest parameters for f1_RED: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for f1_RED: 0.9197378449791203\n",
      "\tBest parameters for f1_YELLOW: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for f1_YELLOW: 0.8540971363793648\n",
      "\tBest parameters for precision_GREEN: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for precision_GREEN: 0.9341092234092023\n",
      "\tBest parameters for precision_RED: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for precision_RED: 0.9240490523968784\n",
      "\tBest parameters for precision_YELLOW: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for precision_YELLOW: 0.8393298059964728\n",
      "\tBest parameters for recall_GREEN: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for recall_GREEN: 0.9003044140030442\n",
      "\tBest parameters for recall_RED: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for recall_RED: 0.9169330289193303\n",
      "\tBest parameters for recall_YELLOW: {'LogisticRegression__C': 10, 'LogisticRegression__max_iter': 100, 'SVC__C': 10, 'SVC__gamma': 'auto', 'SVC__kernel': 'poly'}\n",
      "\tBest score for recall_YELLOW: 0.8724124809741248\n"
     ]
    }
   ],
   "source": [
    "tester_exp1.best_param_calculator_ensamble_from_augmented_data_by_label(cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0132e259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: LogisticRegression() with GridSearchCV\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest parameters for f1: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for f1: 0.7784932321974755\n",
      "\tBest parameters for accuracy: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for accuracy: 0.8383333333333333\n",
      "\tBest parameters for precision: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for precision: 0.7937147306013461\n",
      "\tBest parameters for recall: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for recall: 0.7749398230869344\n",
      "Testing model: SVC(probability=True) with GridSearchCV\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest parameters for f1: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for f1: 0.7888605851373367\n",
      "\tBest parameters for accuracy: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for accuracy: 0.8433333333333334\n",
      "\tBest parameters for precision: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for precision: 0.8038564946706147\n",
      "\tBest parameters for recall: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for recall: 0.7838897050713609\n"
     ]
    }
   ],
   "source": [
    "tester_exp1.best_param_calculator(cv=5,avg='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426de781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: LogisticRegression() with GridSearchCV\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\tBest parameters for f1_GREEN: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for f1_GREEN: 0.9092853147590241\n",
      "\tBest parameters for f1_RED: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for f1_RED: 0.8400423435074513\n",
      "\tBest parameters for f1_YELLOW: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for f1_YELLOW: 0.5861520383259513\n",
      "\tBest parameters for precision_GREEN: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for precision_GREEN: 0.8927130163383803\n",
      "\tBest parameters for precision_RED: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for precision_RED: 0.8234961105305934\n",
      "\tBest parameters for precision_YELLOW: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for precision_YELLOW: 0.6649350649350648\n",
      "\tBest parameters for recall_GREEN: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for recall_GREEN: 0.9278919330289194\n",
      "\tBest parameters for recall_RED: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for recall_RED: 0.8603333333333334\n",
      "\tBest parameters for recall_YELLOW: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for recall_YELLOW: 0.5365942028985506\n",
      "Testing model: SVC(probability=True) with GridSearchCV\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest parameters for f1_GREEN: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for f1_GREEN: 0.9113433346848673\n",
      "\tBest parameters for f1_RED: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for f1_RED: 0.845299938157081\n",
      "\tBest parameters for f1_YELLOW: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for f1_YELLOW: 0.6099384825700616\n",
      "\tBest parameters for precision_GREEN: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for precision_GREEN: 0.8963922857980936\n",
      "\tBest parameters for precision_RED: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for precision_RED: 0.843095238095238\n",
      "\tBest parameters for precision_YELLOW: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for precision_YELLOW: 0.6720819601185125\n",
      "\tBest parameters for recall_GREEN: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for recall_GREEN: 0.9279299847792999\n",
      "\tBest parameters for recall_RED: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for recall_RED: 0.852\n",
      "\tBest parameters for recall_YELLOW: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\tBest score for recall_YELLOW: 0.5717391304347825\n"
     ]
    }
   ],
   "source": [
    "tester_exp1.best_param_calculator_by_label(cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for augmented data with SMOTE\n",
      "Testing model: LogisticRegression() with GridSearchCV\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\tBest parameters for f1: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for f1: 0.823886770582881\n",
      "\tBest parameters for accuracy: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for accuracy: 0.8245349035671616\n",
      "\tBest parameters for precision: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for precision: 0.8271252950207139\n",
      "\tBest parameters for recall: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for recall: 0.8244545915778794\n",
      "Testing model: SVC(probability=True) with GridSearchCV\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest parameters for f1: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for f1: 0.8917843783348254\n",
      "\tBest parameters for accuracy: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for accuracy: 0.8910692951015532\n",
      "\tBest parameters for precision: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for precision: 0.896102520146386\n",
      "\tBest parameters for recall: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for recall: 0.8910958904109588\n",
      "for augmented data with ADASYN\n",
      "Testing model: LogisticRegression() with GridSearchCV\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\tBest parameters for f1: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for f1: 0.823886770582881\n",
      "\tBest parameters for accuracy: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for accuracy: 0.8245349035671616\n",
      "\tBest parameters for precision: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for precision: 0.8271252950207139\n",
      "\tBest parameters for recall: {'C': 10, 'max_iter': 100}\n",
      "\tBest score for recall: 0.8244545915778794\n",
      "Testing model: SVC(probability=True) with GridSearchCV\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest parameters for f1: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for f1: 0.8917843783348254\n",
      "\tBest parameters for accuracy: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for accuracy: 0.8910692951015532\n",
      "\tBest parameters for precision: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for precision: 0.896102520146386\n",
      "\tBest parameters for recall: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for recall: 0.8910958904109588\n"
     ]
    }
   ],
   "source": [
    "tester_exp1.best_param_calculator_from_augmented_data(cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442165e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: LogisticRegression() with GridSearchCV\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\tBest parameters for f1_GREEN: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for f1_GREEN: 0.8806578002974925\n",
      "\tBest parameters for f1_RED: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for f1_RED: 0.8423416194439162\n",
      "\tBest parameters for f1_YELLOW: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for f1_YELLOW: 0.7399670701700666\n",
      "\tBest parameters for precision_GREEN: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for precision_GREEN: 0.893132694910884\n",
      "\tBest parameters for precision_RED: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for precision_RED: 0.8270188578516808\n",
      "\tBest parameters for precision_YELLOW: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for precision_YELLOW: 0.7521124747436952\n",
      "\tBest parameters for recall_GREEN: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for recall_GREEN: 0.8697108066971081\n",
      "\tBest parameters for recall_RED: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for recall_RED: 0.8615296803652969\n",
      "\tBest parameters for recall_YELLOW: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for recall_YELLOW: 0.7339041095890411\n",
      "Testing model: SVC(probability=True) with GridSearchCV\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest parameters for f1_GREEN: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for f1_GREEN: 0.9049470336986787\n",
      "\tBest parameters for f1_RED: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for f1_RED: 0.9141572679151355\n",
      "\tBest parameters for f1_YELLOW: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for f1_YELLOW: 0.8562488333906618\n",
      "\tBest parameters for precision_GREEN: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for precision_GREEN: 0.9535272457011587\n",
      "\tBest parameters for precision_RED: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for precision_RED: 0.9176580935404465\n",
      "\tBest parameters for precision_YELLOW: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for precision_YELLOW: 0.8171222211975533\n",
      "\tBest parameters for recall_GREEN: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for recall_GREEN: 0.8616438356164384\n",
      "\tBest parameters for recall_RED: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for recall_RED: 0.9114535768645358\n",
      "\tBest parameters for recall_YELLOW: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for recall_YELLOW: 0.9001902587519026\n",
      "Testing model: LogisticRegression() with GridSearchCV\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\tBest parameters for f1_GREEN: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for f1_GREEN: 0.8806578002974925\n",
      "\tBest parameters for f1_RED: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for f1_RED: 0.8423416194439162\n",
      "\tBest parameters for f1_YELLOW: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for f1_YELLOW: 0.7399670701700666\n",
      "\tBest parameters for precision_GREEN: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for precision_GREEN: 0.893132694910884\n",
      "\tBest parameters for precision_RED: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for precision_RED: 0.8270188578516808\n",
      "\tBest parameters for precision_YELLOW: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for precision_YELLOW: 0.7521124747436952\n",
      "\tBest parameters for recall_GREEN: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for recall_GREEN: 0.8697108066971081\n",
      "\tBest parameters for recall_RED: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for recall_RED: 0.8615296803652969\n",
      "\tBest parameters for recall_YELLOW: {'C': 1, 'max_iter': 100}\n",
      "\tBest score for recall_YELLOW: 0.7339041095890411\n",
      "Testing model: SVC(probability=True) with GridSearchCV\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBest parameters for f1_GREEN: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for f1_GREEN: 0.9049470336986787\n",
      "\tBest parameters for f1_RED: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for f1_RED: 0.9141572679151355\n",
      "\tBest parameters for f1_YELLOW: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for f1_YELLOW: 0.8562488333906618\n",
      "\tBest parameters for precision_GREEN: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for precision_GREEN: 0.9535272457011587\n",
      "\tBest parameters for precision_RED: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for precision_RED: 0.9176580935404465\n",
      "\tBest parameters for precision_YELLOW: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for precision_YELLOW: 0.8171222211975533\n",
      "\tBest parameters for recall_GREEN: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for recall_GREEN: 0.8616438356164384\n",
      "\tBest parameters for recall_RED: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for recall_RED: 0.9114535768645358\n",
      "\tBest parameters for recall_YELLOW: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\tBest score for recall_YELLOW: 0.9001902587519026\n"
     ]
    }
   ],
   "source": [
    "tester_exp1.best_param_calculator_from_augmented_data_by_label(cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0e6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': {'LogisticRegression': {'f1': {'{\"C\": 0.1, \"max_iter\": 100}': nan},\n",
       "   'accuracy': {'{\"C\": 0.1, \"max_iter\": 100}': 0.8066666666666666},\n",
       "   'precision': {'{\"C\": 0.1, \"max_iter\": 100}': nan},\n",
       "   'recall': {'{\"C\": 0.1, \"max_iter\": 100}': nan}},\n",
       "  'SVC': {'f1': {'{\"C\": 0.1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': nan},\n",
       "   'accuracy': {'{\"C\": 0.1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.8316666666666667},\n",
       "   'precision': {'{\"C\": 0.1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': nan},\n",
       "   'recall': {'{\"C\": 0.1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': nan}},\n",
       "  'StackingClassifier': {'f1': {'{\"LogisticRegression__C\": 0.1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 0.1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.777102240354542},\n",
       "   'accuracy': {'{\"LogisticRegression__C\": 0.1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 0.1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.8416666666666666},\n",
       "   'precision': {'{\"LogisticRegression__C\": 0.1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 0.1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.802230474681455},\n",
       "   'recall': {'{\"LogisticRegression__C\": 0.1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 0.1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.7728367635056139}}},\n",
       " 'specific': {'LogisticRegression': {'f1_GREEN': {'{\"C\": 10, \"max_iter\": 100}': 0.9092853147590241},\n",
       "   'f1_RED': {'{\"C\": 10, \"max_iter\": 100}': 0.8400423435074513},\n",
       "   'f1_YELLOW': {'{\"C\": 10, \"max_iter\": 100}': 0.5861520383259513},\n",
       "   'precision_GREEN': {'{\"C\": 10, \"max_iter\": 100}': 0.8927130163383803},\n",
       "   'precision_RED': {'{\"C\": 10, \"max_iter\": 100}': 0.8234961105305934},\n",
       "   'precision_YELLOW': {'{\"C\": 10, \"max_iter\": 100}': 0.6649350649350648},\n",
       "   'recall_GREEN': {'{\"C\": 10, \"max_iter\": 100}': 0.9278919330289194},\n",
       "   'recall_RED': {'{\"C\": 10, \"max_iter\": 100}': 0.8603333333333334},\n",
       "   'recall_YELLOW': {'{\"C\": 10, \"max_iter\": 100}': 0.5365942028985506}},\n",
       "  'SVC': {'f1_GREEN': {'{\"C\": 1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.9113433346848673},\n",
       "   'f1_RED': {'{\"C\": 1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.845299938157081},\n",
       "   'f1_YELLOW': {'{\"C\": 1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.6099384825700616},\n",
       "   'precision_GREEN': {'{\"C\": 1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.8963922857980936},\n",
       "   'precision_RED': {'{\"C\": 1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.843095238095238},\n",
       "   'precision_YELLOW': {'{\"C\": 1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.6720819601185125},\n",
       "   'recall_GREEN': {'{\"C\": 1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.9279299847792999},\n",
       "   'recall_RED': {'{\"C\": 1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.852},\n",
       "   'recall_YELLOW': {'{\"C\": 1, \"gamma\": \"scale\", \"kernel\": \"linear\"}': 0.5717391304347825}},\n",
       "  'StackingClassifier': {'f1_GREEN': {'{\"LogisticRegression__C\": 1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.9101078741337009},\n",
       "   'f1_RED': {'{\"LogisticRegression__C\": 1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.8389999999999999},\n",
       "   'f1_YELLOW': {'{\"LogisticRegression__C\": 1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.5712039072039071},\n",
       "   'precision_GREEN': {'{\"LogisticRegression__C\": 1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.8847615018798203},\n",
       "   'precision_RED': {'{\"LogisticRegression__C\": 1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.8127435897435898},\n",
       "   'precision_YELLOW': {'{\"LogisticRegression__C\": 1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.6893162393162393},\n",
       "   'recall_GREEN': {'{\"LogisticRegression__C\": 1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.9390030441400304},\n",
       "   'recall_RED': {'{\"LogisticRegression__C\": 1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.869},\n",
       "   'recall_YELLOW': {'{\"LogisticRegression__C\": 1, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 1, \"SVC__gamma\": \"scale\", \"SVC__kernel\": \"linear\"}': 0.5018115942028986}}},\n",
       " 'overall_SMOTE': {'LogisticRegression': {'f1': {'{\"C\": 10, \"max_iter\": 100}': 0.823886770582881},\n",
       "   'accuracy': {'{\"C\": 10, \"max_iter\": 100}': 0.8245349035671616},\n",
       "   'precision': {'{\"C\": 10, \"max_iter\": 100}': 0.8271252950207139},\n",
       "   'recall': {'{\"C\": 10, \"max_iter\": 100}': 0.8244545915778794}},\n",
       "  'SVC': {'f1': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.8917843783348254},\n",
       "   'accuracy': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.8910692951015532},\n",
       "   'precision': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.896102520146386},\n",
       "   'recall': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.8910958904109588}}},\n",
       " 'specific_SMOTE': {'LogisticRegression': {'f1_GREEN': {'{\"C\": 1, \"max_iter\": 100}': 0.8806578002974925},\n",
       "   'f1_RED': {'{\"C\": 1, \"max_iter\": 100}': 0.8423416194439162},\n",
       "   'f1_YELLOW': {'{\"C\": 1, \"max_iter\": 100}': 0.7399670701700666},\n",
       "   'precision_GREEN': {'{\"C\": 1, \"max_iter\": 100}': 0.893132694910884},\n",
       "   'precision_RED': {'{\"C\": 1, \"max_iter\": 100}': 0.8270188578516808},\n",
       "   'precision_YELLOW': {'{\"C\": 1, \"max_iter\": 100}': 0.7521124747436952},\n",
       "   'recall_GREEN': {'{\"C\": 1, \"max_iter\": 100}': 0.8697108066971081},\n",
       "   'recall_RED': {'{\"C\": 1, \"max_iter\": 100}': 0.8615296803652969},\n",
       "   'recall_YELLOW': {'{\"C\": 1, \"max_iter\": 100}': 0.7339041095890411}},\n",
       "  'SVC': {'f1_GREEN': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.9049470336986787},\n",
       "   'f1_RED': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.9141572679151355},\n",
       "   'f1_YELLOW': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.8562488333906618},\n",
       "   'precision_GREEN': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.9535272457011587},\n",
       "   'precision_RED': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.9176580935404465},\n",
       "   'precision_YELLOW': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.8171222211975533},\n",
       "   'recall_GREEN': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.8616438356164384},\n",
       "   'recall_RED': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.9114535768645358},\n",
       "   'recall_YELLOW': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.9001902587519026}},\n",
       "  'StackingClassifier': {'f1_GREEN': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.916383317106266},\n",
       "   'f1_RED': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.9169952555306375},\n",
       "   'f1_YELLOW': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.8516679786160747},\n",
       "   'precision_GREEN': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.9365315350568661},\n",
       "   'precision_RED': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.9214291857770119},\n",
       "   'precision_YELLOW': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.8344365819177003},\n",
       "   'recall_GREEN': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.8975266362252663},\n",
       "   'recall_RED': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.9141933028919331},\n",
       "   'recall_YELLOW': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.8724124809741248}}},\n",
       " 'overall_ADASYN': {'LogisticRegression': {'f1': {'{\"C\": 10, \"max_iter\": 100}': 0.823886770582881},\n",
       "   'accuracy': {'{\"C\": 10, \"max_iter\": 100}': 0.8245349035671616},\n",
       "   'precision': {'{\"C\": 10, \"max_iter\": 100}': 0.8271252950207139},\n",
       "   'recall': {'{\"C\": 10, \"max_iter\": 100}': 0.8244545915778794}},\n",
       "  'SVC': {'f1': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.8917843783348254},\n",
       "   'accuracy': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.8910692951015532},\n",
       "   'precision': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.896102520146386},\n",
       "   'recall': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.8910958904109588}}},\n",
       " 'specific_ADASYN': {'LogisticRegression': {'f1_GREEN': {'{\"C\": 1, \"max_iter\": 100}': 0.8806578002974925},\n",
       "   'f1_RED': {'{\"C\": 1, \"max_iter\": 100}': 0.8423416194439162},\n",
       "   'f1_YELLOW': {'{\"C\": 1, \"max_iter\": 100}': 0.7399670701700666},\n",
       "   'precision_GREEN': {'{\"C\": 1, \"max_iter\": 100}': 0.893132694910884},\n",
       "   'precision_RED': {'{\"C\": 1, \"max_iter\": 100}': 0.8270188578516808},\n",
       "   'precision_YELLOW': {'{\"C\": 1, \"max_iter\": 100}': 0.7521124747436952},\n",
       "   'recall_GREEN': {'{\"C\": 1, \"max_iter\": 100}': 0.8697108066971081},\n",
       "   'recall_RED': {'{\"C\": 1, \"max_iter\": 100}': 0.8615296803652969},\n",
       "   'recall_YELLOW': {'{\"C\": 1, \"max_iter\": 100}': 0.7339041095890411}},\n",
       "  'SVC': {'f1_GREEN': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.9049470336986787},\n",
       "   'f1_RED': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.9141572679151355},\n",
       "   'f1_YELLOW': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.8562488333906618},\n",
       "   'precision_GREEN': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.9535272457011587},\n",
       "   'precision_RED': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.9176580935404465},\n",
       "   'precision_YELLOW': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.8171222211975533},\n",
       "   'recall_GREEN': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.8616438356164384},\n",
       "   'recall_RED': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.9114535768645358},\n",
       "   'recall_YELLOW': {'{\"C\": 10, \"gamma\": \"scale\", \"kernel\": \"poly\"}': 0.9001902587519026}},\n",
       "  'StackingClassifier': {'f1_GREEN': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.9166155022218897},\n",
       "   'f1_RED': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.9197378449791203},\n",
       "   'f1_YELLOW': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.8540971363793648},\n",
       "   'precision_GREEN': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.9341092234092023},\n",
       "   'precision_RED': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.9240490523968784},\n",
       "   'precision_YELLOW': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.8393298059964728},\n",
       "   'recall_GREEN': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.9003044140030442},\n",
       "   'recall_RED': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.9169330289193303},\n",
       "   'recall_YELLOW': {'{\"LogisticRegression__C\": 10, \"LogisticRegression__max_iter\": 100, \"SVC__C\": 10, \"SVC__gamma\": \"auto\", \"SVC__kernel\": \"poly\"}': 0.8724124809741248}}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester_exp1.performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e6738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Model</th>\n",
       "      <th>Metric/Class</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>Performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>overall</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>f1</td>\n",
       "      <td>{\"C\": 10, \"max_iter\": 100}</td>\n",
       "      <td>0.778493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>overall</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>{\"C\": 10, \"max_iter\": 100}</td>\n",
       "      <td>0.838333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overall</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>precision</td>\n",
       "      <td>{\"C\": 10, \"max_iter\": 100}</td>\n",
       "      <td>0.793715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>overall</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>recall</td>\n",
       "      <td>{\"C\": 10, \"max_iter\": 100}</td>\n",
       "      <td>0.774940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>overall</td>\n",
       "      <td>SVC</td>\n",
       "      <td>f1</td>\n",
       "      <td>{\"C\": 1, \"gamma\": \"scale\", \"kernel\": \"linear\"}</td>\n",
       "      <td>0.788861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>specific_ADASYN</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>precision_RED</td>\n",
       "      <td>{\"LogisticRegression__C\": 10, \"LogisticRegress...</td>\n",
       "      <td>0.924049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>specific_ADASYN</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>precision_YELLOW</td>\n",
       "      <td>{\"LogisticRegression__C\": 10, \"LogisticRegress...</td>\n",
       "      <td>0.839330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>specific_ADASYN</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>recall_GREEN</td>\n",
       "      <td>{\"LogisticRegression__C\": 10, \"LogisticRegress...</td>\n",
       "      <td>0.900304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>specific_ADASYN</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>recall_RED</td>\n",
       "      <td>{\"LogisticRegression__C\": 10, \"LogisticRegress...</td>\n",
       "      <td>0.916933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>specific_ADASYN</td>\n",
       "      <td>StackingClassifier</td>\n",
       "      <td>recall_YELLOW</td>\n",
       "      <td>{\"LogisticRegression__C\": 10, \"LogisticRegress...</td>\n",
       "      <td>0.872412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Experiment               Model      Metric/Class  \\\n",
       "0            overall  LogisticRegression                f1   \n",
       "1            overall  LogisticRegression          accuracy   \n",
       "2            overall  LogisticRegression         precision   \n",
       "3            overall  LogisticRegression            recall   \n",
       "4            overall                 SVC                f1   \n",
       "..               ...                 ...               ...   \n",
       "104  specific_ADASYN  StackingClassifier     precision_RED   \n",
       "105  specific_ADASYN  StackingClassifier  precision_YELLOW   \n",
       "106  specific_ADASYN  StackingClassifier      recall_GREEN   \n",
       "107  specific_ADASYN  StackingClassifier        recall_RED   \n",
       "108  specific_ADASYN  StackingClassifier     recall_YELLOW   \n",
       "\n",
       "                                       Hyperparameters  Performance  \n",
       "0                           {\"C\": 10, \"max_iter\": 100}     0.778493  \n",
       "1                           {\"C\": 10, \"max_iter\": 100}     0.838333  \n",
       "2                           {\"C\": 10, \"max_iter\": 100}     0.793715  \n",
       "3                           {\"C\": 10, \"max_iter\": 100}     0.774940  \n",
       "4       {\"C\": 1, \"gamma\": \"scale\", \"kernel\": \"linear\"}     0.788861  \n",
       "..                                                 ...          ...  \n",
       "104  {\"LogisticRegression__C\": 10, \"LogisticRegress...     0.924049  \n",
       "105  {\"LogisticRegression__C\": 10, \"LogisticRegress...     0.839330  \n",
       "106  {\"LogisticRegression__C\": 10, \"LogisticRegress...     0.900304  \n",
       "107  {\"LogisticRegression__C\": 10, \"LogisticRegress...     0.916933  \n",
       "108  {\"LogisticRegression__C\": 10, \"LogisticRegress...     0.872412  \n",
       "\n",
       "[109 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester.make_dataframe_performances(tester.performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14936ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
